{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example from: Recurrent Neural Networks in Tensorflow II\n",
    "\n",
    "http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## <font color='purple'>We will use LSTM and GRU </font>\n",
    "* ## <font color='purple'>Simple NLP Task: character-level language model to generate character sequences </font>\n",
    "### a la Andrej Karpathy’s char-rnn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'> We’ll use the tiny-shakespeare corpus as our data, though we could use any plain text file.\n",
    "* We’ll choose to use all of the characters in the text file as our vocabulary, treating lowercase and capital letters are separate characters. </font>\n",
    "\n",
    "Additionally, it is likely a good idea to restrict the vocabulary (i.e., the set of characters) used, by replacing uncommon characters with an UNK token (like a square: □)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.models.rnn.ptb import reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\r\n",
      "Version: 0.10.0\r\n",
      "Summary: TensorFlow helps the tensors flow\r\n",
      "Home-page: http://tensorflow.org/\r\n",
      "Author: Google Inc.\r\n",
      "Author-email: opensource@google.com\r\n",
      "License: Apache 2.0\r\n",
      "Location: /gpfs/global_fs01/sym_shared/YPProdSpark/user/s35d-c92fe439b3aa21-1525a0f645bc/.local/lib/python2.7/site-packages\r\n",
      "Requires: mock, numpy, six, protobuf, wheel\r\n"
     ]
    }
   ],
   "source": [
    "! pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'>Update tensorflow to 0.10.0 for managing state_is_tuple=True in LSTM (see below) </font>\n",
    "\n",
    "* ### cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "* ### cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>The task:  generate character sequences</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and data\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "\n",
    "#file_url = 'http://latel.upf.edu/traductica/scp/quijote/quijote.txt'\n",
    "#file_name = 'cervantes.txt'\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data length:', 1115394)\n"
     ]
    }
   ],
   "source": [
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print raw_data[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## understand : vocab : unique elements in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['\\n', '!', ' ', '$', \"'\", '&', '-', ',', '.', '3', ';', ':', '?', 'A', 'C', 'B', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'J', 'M', 'L', 'O', 'N', 'Q', 'P', 'S', 'R', 'U', 'T', 'W', 'V', 'Y', 'X', 'Z', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n"
     ]
    }
   ],
   "source": [
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: '!',\n",
       " 2: ' ',\n",
       " 3: '$',\n",
       " 4: \"'\",\n",
       " 5: '&',\n",
       " 6: '-',\n",
       " 7: ',',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ';',\n",
       " 11: ':',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'C',\n",
       " 15: 'B',\n",
       " 16: 'E',\n",
       " 17: 'D',\n",
       " 18: 'G',\n",
       " 19: 'F',\n",
       " 20: 'I',\n",
       " 21: 'H',\n",
       " 22: 'K',\n",
       " 23: 'J',\n",
       " 24: 'M',\n",
       " 25: 'L',\n",
       " 26: 'O',\n",
       " 27: 'N',\n",
       " 28: 'Q',\n",
       " 29: 'P',\n",
       " 30: 'S',\n",
       " 31: 'R',\n",
       " 32: 'U',\n",
       " 33: 'T',\n",
       " 34: 'W',\n",
       " 35: 'V',\n",
       " 36: 'Y',\n",
       " 37: 'X',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'c',\n",
       " 41: 'b',\n",
       " 42: 'e',\n",
       " 43: 'd',\n",
       " 44: 'g',\n",
       " 45: 'f',\n",
       " 46: 'i',\n",
       " 47: 'h',\n",
       " 48: 'k',\n",
       " 49: 'j',\n",
       " 50: 'm',\n",
       " 51: 'l',\n",
       " 52: 'o',\n",
       " 53: 'n',\n",
       " 54: 'q',\n",
       " 55: 'p',\n",
       " 56: 's',\n",
       " 57: 'r',\n",
       " 58: 'u',\n",
       " 59: 't',\n",
       " 60: 'w',\n",
       " 61: 'v',\n",
       " 62: 'y',\n",
       " 63: 'x',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_idx['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 2,\n",
       " '!': 1,\n",
       " '$': 3,\n",
       " '&': 5,\n",
       " \"'\": 4,\n",
       " ',': 7,\n",
       " '-': 6,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 11,\n",
       " ';': 10,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 15,\n",
       " 'C': 14,\n",
       " 'D': 17,\n",
       " 'E': 16,\n",
       " 'F': 19,\n",
       " 'G': 18,\n",
       " 'H': 21,\n",
       " 'I': 20,\n",
       " 'J': 23,\n",
       " 'K': 22,\n",
       " 'L': 25,\n",
       " 'M': 24,\n",
       " 'N': 27,\n",
       " 'O': 26,\n",
       " 'P': 29,\n",
       " 'Q': 28,\n",
       " 'R': 31,\n",
       " 'S': 30,\n",
       " 'T': 33,\n",
       " 'U': 32,\n",
       " 'V': 35,\n",
       " 'W': 34,\n",
       " 'X': 37,\n",
       " 'Y': 36,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 41,\n",
       " 'c': 40,\n",
       " 'd': 43,\n",
       " 'e': 42,\n",
       " 'f': 45,\n",
       " 'g': 44,\n",
       " 'h': 47,\n",
       " 'i': 46,\n",
       " 'j': 49,\n",
       " 'k': 48,\n",
       " 'l': 51,\n",
       " 'm': 50,\n",
       " 'n': 53,\n",
       " 'o': 52,\n",
       " 'p': 55,\n",
       " 'q': 54,\n",
       " 'r': 57,\n",
       " 's': 56,\n",
       " 't': 59,\n",
       " 'u': 58,\n",
       " 'v': 61,\n",
       " 'w': 60,\n",
       " 'x': 63,\n",
       " 'y': 62,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### understand: converting text data into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [vocab_to_idx[c] for c in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 46, 57, 56, 59, 2, 14, 46, 59, 46]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recover_data = [idx_to_vocab[c] for c in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del recover_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'> Some utility functions for feeding batches</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PTB from Penn Tree Bank (PTB) dataset\n",
    "\n",
    "<font color='green'>reader.ptb_iterator(data, batch_size, num_steps)<7font>\n",
    "\n",
    "https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \n",
    "  Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  \n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    \n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    \n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "    \n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch no. =', 0)\n",
      "('Total no. steps=', 173)\n",
      "X information....\n",
      "(32, 200)\n",
      "Y information....\n",
      "(32, 200)\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "num_steps=200\n",
    "batch_size=32\n",
    "\n",
    "Xepoch=gen_epochs(num_epochs, num_steps, batch_size)\n",
    "\n",
    "for idx, epoch in enumerate(Xepoch):\n",
    "    print('epoch no. =',idx)\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "            cc=0\n",
    "    \n",
    "    print(\"Total no. steps=\",step)\n",
    "    print \"X information....\"\n",
    "    print(X.shape)\n",
    "    print \"Y information....\"\n",
    "    print(Y.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "173*200*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1115394/(32*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch no. =', 0)\n",
      "0\n",
      "X information....\n",
      "(32, 200)\n",
      "<type 'numpy.ndarray'>\n",
      "[[19 46 57 ..., 62 52 58]\n",
      " [45 52 57 ..., 50 39 62]\n",
      " [ 2 14 52 ..., 19 46 57]\n",
      " ..., \n",
      " [52 58  2 ..., 58 56 55]\n",
      " [ 2 47 52 ..., 45  2 60]\n",
      " [ 2 45 52 ..., 52 56 42]]\n",
      "Y information....\n",
      "(32, 200)\n",
      "<type 'numpy.ndarray'>\n",
      "[[46 57 56 ..., 52 58  2]\n",
      " [52 57  2 ..., 39 62  2]\n",
      " [14 52 57 ..., 46 57 56]\n",
      " ..., \n",
      " [58  2 60 ..., 56 55 46]\n",
      " [47 52 60 ...,  2 60 52]\n",
      " [45 52 52 ..., 56 42  2]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "num_steps=200\n",
    "batch_size=32\n",
    "\n",
    "Xepoch=gen_epochs(num_epochs, num_steps, batch_size)\n",
    "\n",
    "for idx, epoch in enumerate(Xepoch):\n",
    "    print('epoch no. =',idx)\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "        if step % 500 == 0:\n",
    "            print(step)\n",
    "            print \"X information....\"\n",
    "            print(X.shape)\n",
    "            print(type(X))\n",
    "            print(X[0:10])\n",
    "            print \"Y information....\"\n",
    "            print(Y.shape)\n",
    "            print(type(Y))\n",
    "            print(Y[0:10])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for graph reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: RNN graph definition</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "num_classes = vocab_size\n",
    "batch_size = 32\n",
    "num_steps = 200\n",
    "num_layers = 3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## <font color='red'>Create EMBEDDINGS</font>\n",
    "http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "# Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#LSTM\n",
    "#cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "# GRU\n",
    "cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train RNN (LSTM or GRU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average training loss for Epoch', 0, ':', 2.4570799510121994)\n",
      "('Average training loss for Epoch', 1, ':', 2.4335354098264759)\n",
      "('Average training loss for Epoch', 2, ':', 2.4350139231010357)\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "verbose=True\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "tf.set_random_seed(2345)\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "#                if training_state is not None:\n",
    "#                    feed_dict[g['init_state']] = training_state\n",
    " \n",
    "                training_loss_, training_state, _ = sess.run([total_loss,\n",
    "                                                      final_state,\n",
    "                                                      train_step],\n",
    "                                                          feed_dict={x: X, y: Y})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\"for last 100 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/steps)\n",
    "                    training_loss = 0\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            \n",
    "        saver = tf.train.Saver()\n",
    "        #saver.save(sess, 'RNN_GRU_model_cervantes')\n",
    "        saver.save(sess, 'RNN_GRU_model_shakespeare')\n",
    "            \n",
    "print(\"It took\", time.time() - t, \"seconds this training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some results:\n",
    "\n",
    "### LSTM\n",
    "* cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "* cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "+ ('Average training loss for Epoch', 0, ':', 3.5619977603013488)\n",
    "+ ('It took', 38.50640511512756, 'seconds this training.')\n",
    "\n",
    "### GRU\n",
    "* ('Average training loss for Epoch', 0, ':', 3.6105946466840546)\n",
    "* ('It took', 35.258342027664185, 'seconds this training.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working directory to save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RNN_GRU_model', 'RNN_GRU_model.meta', 'tinyshakespeare.txt', 'RNN_GRU_model_shakespeare.meta', 'checkpoint', 'RNN_GRU_model_shakespeare', 'cervantes.txt']\n"
     ]
    }
   ],
   "source": [
    "print os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove dir not empty + sub dirs\n",
    "#import shutil\n",
    "\n",
    "#shutil.rmtree('./Ubi_Voice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: models (LSTM or GRU) can be used to generate TEXT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## First: <font size='3'>we need to rebuild the graph so as to accept a single character at a time</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "num_classes = vocab_size\n",
    "batch_size = 1\n",
    "num_steps = 1\n",
    "num_layers = 3\n",
    "learning_rate = 1e-4\n",
    "num_epochs=1\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "# Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    \n",
    "#LSTM\n",
    "#cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "# GRU\n",
    "cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then:\n",
    "* <font size='3'>Restore our saved model.\n",
    "* We’ll give the network a single character prompt,i.e. prompt='A'\n",
    "* Grab its predicted probability distribution for the next character\n",
    "* Use that distribution to pick the next character, and repeat. </font>\n",
    "###   \n",
    "<font size='3'>When picking the next character, using pick_top_chars != None to use the whole probability distribution (default), or be forced to pick one of the top n most likely characters in the distribution. The latter option should obtain more English-like results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And art and the wime, wand that ande, thend thas and wo that thes wont mant se hen wat,\n",
      "To hort momes to sirthind hen as that ton home hes wanghe arthy home, wo dowe hor wingers and tho hath sore this whar sones shee hat my wore tore and, the sore thor and and sees to me sill sead and and ant has se so hater hist, thow he with thin tome wand and sont that sas shall ande, wore treare ast thes and when that thint, that sirist hear, so mente and the whe hith ast an shes\n",
      "Tille the him so that thine at myot a hand and thes weand sint meat hesere wine thest at a doun that to hes were thine thens, and that me woned shat sill that that ate wast this his to toul therst the with, that ho doulllle thath malint ho thith shat mith my son werd somand sort\n"
     ]
    }
   ],
   "source": [
    "prompt='A'\n",
    "pick_top_chars=5\n",
    "num_chars=750\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, \"RNN_GRU_model_shakespeare\")\n",
    "    \n",
    "    state = None\n",
    "    current_char = vocab_to_idx[prompt]\n",
    "    chars = [current_char]\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                preds, state = sess.run([predictions,final_state], feed_dict={x: [[current_char]], init_state: state})\n",
    "            else:\n",
    "                preds, state = sess.run([predictions,final_state], feed_dict={x: [[current_char]]})\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "            \n",
    "chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "print(\"\".join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}