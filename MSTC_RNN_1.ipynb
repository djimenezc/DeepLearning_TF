{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks in Tensorflow I\n",
    "\n",
    "### from:\n",
    "### http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <font color='green'>Outline of the data</font>\n",
    "\n",
    "In this post, we’ll be building a no frills RNN that accepts a binary sequence X and uses it to predict a binary sequence Y. The sequences are constructed as follows:\n",
    "\n",
    "    Input sequence (X): At time step t, Xt has a 50% chance of being 1 (and a 50% chance of being 0). E.g., X might be [1, 0, 0, 1, 1, 1 … ].\n",
    "    \n",
    "    Output sequence (Y): At time step t, Yt has a base 50% chance of being 1 (and a 50% base chance to be 0). The \n",
    "    chance of Yt being 1 is increased by 50% (i.e., to 100%) if Xt−3 is 1, and decreased by 25% (i.e., to 25%) if Xt−8 is 1. If both Xt−3 and Xt−8 are 1, the chance of Yt being 1 is 50% + 50% - 25% = 75%.\n",
    "\n",
    "Thus, there are two dependencies in the data: one at t-3 (3 steps back) and one at t-8 (8 steps back)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected cross-entropy\n",
    "\n",
    "This data is simple enough that we can calculate the expected cross-entropy loss for a trained RNN depending on whether or not it learns the dependencies:\n",
    "\n",
    "* If the network learns no dependencies, it will correctly assign a probability of 62.5% to 1, for an expected cross-entropy loss of about 0.66.\n",
    "* If the network learns only the first dependency (3 steps back) but not the second dependency, it will correctly assign a probability of 87.5%, 50% of the time, and correctly assign a probability of 62.5% the other 50% of the time, for an expected cross entropy loss of about 0.52.\n",
    "* If the network learns both dependencies, it will be 100% accurate 25% of the time, correctly assign a probability of 50%, 25% of the time, and correctly assign a probability of 75%, 50% of the time, for an expected cross extropy loss of about 0.45.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "('- learns neither dependency:', 0.66156323815798213)\n",
      "('- learns first dependency:  ', 0.51916669970720941)\n",
      "('- learns both dependencies: ', 0.4544543674493905)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>First: dealing with data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Generate our binary sequences**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Prepare for feeding data into the graph:** *from raw data to batches*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: we will start with a vanilla RNN model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://r2rt.com/static/images/BasicRNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and we will use static tf.nn.rnn so it uses lists of tensors to represent the width = num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The easiest way to represent each type of duplicate tensor (the rnn inputs, the rnn outputs (hidden state), the predictions, and the loss) is as a list of tensors\n",
    "\n",
    "<img src=\"http://r2rt.com/static/images/BasicRNNLabeled.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Set GLOBAL Configuration Variables**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_epochs=1\n",
    "num_steps = 5 # number of truncated backprop steps\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 8\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Defining the graph model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs : modifications due to one-ho-encoding problems:\n",
    "\n",
    "  1  !!! TRANSPOSE FOR THIS!!! nn_inputs is a \n",
    "      list of num_steps tensors with shape [batch_size, num_classe\n",
    "  2.,. MUST be int64 !!!\n",
    "  3.- add 1,0 (ONE HOT Using  0 ... 1)\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "# NEED tocast to int64\n",
    "# ORIGINAL x_one_hot = tf.one_hot(x, num_classes)\n",
    "\n",
    "\n",
    "x_one_hot = tf.one_hot(tf.cast(tf.transpose(x, perm=[1, 0]), tf.int64), num_classes, 1,0)\n",
    "\n",
    "rnn_inputs = tf.unpack(tf.cast(x_one_hot, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Definition of a BasicRNNCell**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Adding run_cells to graph**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Output y(t) (softmax) Logits and Predictions**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses and total_loss are simlar to the \"sequence_loss_by_example\" and \"sequence_loss\"\n",
    "functions, respectively, from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Loss function and training step**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn our y placeholder into a list labels\n",
    "y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit,label) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "losses_last=losses[-1]\n",
    "total_loss = tf.reduce_mean(losses[-1])\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train the network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Function to train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        print(\"Starting for idx...\")\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses_last,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for idx...\n",
      "('\\nEPOCH', 0)\n",
      "('Average loss at step', 100, 'for last 250 steps:', 0.53080026358366017)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.49509493201971055)\n",
      "('Average loss at step', 300, 'for last 250 steps:', 0.48537441760301592)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.48419511228799822)\n",
      "('Average loss at step', 500, 'for last 250 steps:', 0.48765994429588316)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.48771368116140368)\n",
      "('Average loss at step', 700, 'for last 250 steps:', 0.4894577404856682)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.48612576514482497)\n",
      "('Average loss at step', 900, 'for last 250 steps:', 0.48130953043699265)\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(num_epochs,num_steps,state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Plotting training losses**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f70a4115c50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70c7335f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: improve the model playing with hyperparameters num_steps state_size</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**And try to understand your results!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then...see next Notebook ...translating our model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}